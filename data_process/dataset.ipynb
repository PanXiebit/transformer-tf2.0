{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset  dataset.ipynb\tdataset.py  en.subwords\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51785 1193\n"
     ]
    }
   ],
   "source": [
    "examples, metadata = tfds.load(name=\"ted_hrlr_translate/pt_to_en\",\n",
    "                               with_info=True,\n",
    "                               as_supervised=True,\n",
    "                               data_dir=\"./dataset/\")\n",
    "train_examples, val_examples = examples[\"train\"], examples[\"validation\"]\n",
    "print(len(list(train_examples)), len(list(val_examples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'os astr\\xc3\\xb3nomos acreditam que cada estrela da gal\\xc3\\xa1xia tem um planeta , e especulam que at\\xc3\\xa9 um quinto deles tem um planeta do tipo da terra que poder\\xc3\\xa1 ter vida , mas ainda n\\xc3\\xa3o vimos nenhum deles .', shape=(), dtype=string)\n",
      "b'os astr\\xc3\\xb3nomos acreditam que cada estrela da gal\\xc3\\xa1xia tem um planeta , e especulam que at\\xc3\\xa9 um quinto deles tem um planeta do tipo da terra que poder\\xc3\\xa1 ter vida , mas ainda n\\xc3\\xa3o vimos nenhum deles .'\n",
      "tf.Tensor(b\"astronomers now believe that every star in the galaxy has a planet , and they speculate that up to one fifth of them have an earth-like planet that might be able to harbor life , but we have n't seen any of them .\", shape=(), dtype=string)\n",
      "b\"astronomers now believe that every star in the galaxy has a planet , and they speculate that up to one fifth of them have an earth-like planet that might be able to harbor life , but we have n't seen any of them .\"\n"
     ]
    }
   ],
   "source": [
    "for i, (pt, en) in enumerate(train_examples):\n",
    "    if i > 0:\n",
    "        break\n",
    "    print(pt)\n",
    "    print(pt.numpy())\n",
    "    print(en)\n",
    "    print(en.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 迭代器 \n",
    "train_examples_small = iter(list(train_examples)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en_small = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    corpus_generator=(en.numpy() for pt, en in train_examples_small),   # generator yielding `str`\n",
    "    target_vocab_size=2 ** 13,\n",
    "    max_subword_length=20,\n",
    "    max_corpus_chars=None,\n",
    "    reserved_tokens=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[162, 192, 175, 188, 193, 180, 189, 192, 187, 179, 192, 110, 183, 193, 110, 175, 197, 179, 29, 187, 179, 124]\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Transformer is awesome.'\n",
    "print(tokenizer_en_small.encode(sample_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [7915, 1248, 7946, 7194, 13, 2799, 7877]\n",
      "The original string: Transformer is awesome.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    corpus_generator=(en.numpy() for pt, en in train_examples),   # generator yielding `str`\n",
    "    target_vocab_size=2 ** 13,\n",
    "    max_subword_length=20,\n",
    "    max_corpus_chars=None,\n",
    "    reserved_tokens=None\n",
    ")\n",
    "\n",
    "tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "sample_string = 'Transformer is awesome.'\n",
    "\n",
    "tokenized_string = tokenizer_en.encode(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer_en.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en.save_to_file(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset  dataset.ipynb\tdataset.py  en.subwords\n",
      "7832 en.subwords\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "!wc -l en.subwords  # 奇怪为什么 subword　的词表大小不一样呢。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7915 ----> T\n",
      "1248 ----> ran\n",
      "7946 ----> s\n",
      "7194 ----> former \n",
      "13 ----> is \n",
      "2799 ----> awesome\n",
      "7877 ----> .\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "    print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenizer.encode 是把 string 先做 bpe,　分割成 subwords,　然后转换成　index  \n",
    "- tokenizer.decode 是把 index 转换成　subwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a start and end token to the input and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8087, 8214)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.vocab_size, tokenizer_pt.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(lang1, lang2):\n",
    "    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "      lang1.numpy()) + [tokenizer_pt.vocab_size+1]   # add start and end token\n",
    "\n",
    "    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "\n",
    "    return lang1, lang2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_example = 'Transformer é incrível.'\n",
    "en_example = 'Transformer is awesome.'\n",
    "pt_example = tf.convert_to_tensor(pt_example, dtype=tf.string)\n",
    "en_example = tf.convert_to_tensor(en_example, dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8214, 8042, 2883, 8073, 2266, 383, 8, 1917, 8004, 8215]\n",
      "[8087, 7915, 1248, 7946, 7194, 13, 2799, 7877, 8088]\n"
     ]
    }
   ],
   "source": [
    "pt_enc, en_enc = encode(pt_example, en_example)\n",
    "print(pt_enc)\n",
    "print(en_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 tf.logical_and 删除掉长度超过 40 的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(10, shape=(), dtype=int32) tf.Tensor(True, shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "print(tf.size(pt_enc), tf.size(pt_enc) <= 40) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=520474, shape=(), dtype=bool, numpy=True>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.logical_and(tf.size(pt_enc) <= MAX_LENGTH,\n",
    "             tf.size(en_enc) <= MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_encode(pt, en):\n",
    "    return tf.py_function(encode, [pt, en], [tf.int64, tf.int64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0509 17:47:57.726433 140119709894464 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n",
      "W0509 17:47:57.728044 140119709894464 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=520482, shape=(10,), dtype=int64, numpy=array([8214, 8042, 2883, 8073, 2266,  383,    8, 1917, 8004, 8215])>,\n",
       " <tf.Tensor: id=520483, shape=(9,), dtype=int64, numpy=array([8087, 7915, 1248, 7946, 7194,   13, 2799, 7877, 8088])>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_encode(pt_example, en_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: (<unknown>, <unknown>), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FilterDataset shapes: (<unknown>, <unknown>), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(\n",
    "    BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(\n",
    "    BATCH_SIZE, padded_shapes=([-1], [-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "703\n"
     ]
    }
   ],
   "source": [
    "print(len(list(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0509 17:51:00.207847 140117419202304 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n",
      "W0509 17:51:00.208775 140117419202304 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n",
      "W0509 17:51:00.210186 140117419202304 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=521021, shape=(64, 40), dtype=int64, numpy=\n",
       " array([[8214, 1259,    5, ...,    0,    0,    0],\n",
       "        [8214,  299,   13, ...,    0,    0,    0],\n",
       "        [8214,   59,    8, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8214,   95,    3, ...,    0,    0,    0],\n",
       "        [8214, 5157,    1, ...,    0,    0,    0],\n",
       "        [8214, 4479, 7990, ...,    0,    0,    0]])>,\n",
       " <tf.Tensor: id=521022, shape=(64, 40), dtype=int64, numpy=\n",
       " array([[8087,   18,   12, ...,    0,    0,    0],\n",
       "        [8087,  634,   30, ...,    0,    0,    0],\n",
       "        [8087,   16,   13, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8087,   12,   20, ...,    0,    0,    0],\n",
       "        [8087,   17, 4981, ...,    0,    0,    0],\n",
       "        [8087,   12, 5453, ...,    0,    0,    0]])>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_batch, en_batch = next(iter(val_dataset))\n",
    "pt_batch, en_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-tf2",
   "language": "python",
   "name": "py3-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
