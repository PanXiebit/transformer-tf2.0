{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training\n",
    "\n",
    "在训练阶段，target既要作为 decoder 的输入(tgt_inp)，也要作为 label(tgt_real).但是两者有一定的区别，tar_real 是 tar_inp 左移一个位置之后的结果。对于同一个 location，tgt_real 对应的是 tag_inp 需要预测的下一个位置的 token。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, sentence = \"SOS A lion in the jungle is sleeping EOS\"\n",
    "\n",
    "tar_inp = \"SOS A lion in the jungle is sleeping\"\n",
    "\n",
    "tar_real = \"A lion in the jungle is sleeping EOS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### teaching forcing\n",
    "\n",
    "During training this example uses teacher-forcing (like in the text generation tutorial). Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.\n",
    "\n",
    "As the transformer predicts each word, self-attention allows it to look at the previous words in the input sequence to better predict the next word.\n",
    "\n",
    "To prevent the model from peaking at the expected output the model uses a look-ahead mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "tar_inp = tar[:, :-1]\n",
    "tar_real = tar[:, 1:]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientTape\n",
    "\n",
    "https://www.tensorflow.org/guide/eager?hl=zh-cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算梯度\n",
    "\n",
    "自动微分对于实现机器学习算法（例如用于训练神经网络的反向传播）来说很有用。在 Eager Execution 期间，请使用 tf.GradientTape 跟踪操作以便稍后计算梯度。\n",
    "\n",
    "tf.GradientTape 是一种选择性功能，可在不跟踪时提供最佳性能。由于在每次调用期间都可能发生不同的操作，因此所有前向传播操作都会记录到“磁带”中。要计算梯度，请反向播放磁带，然后放弃。特定的 tf.GradientTape 只能计算一个梯度；随后的调用会抛出运行时错误。\n",
    "\n",
    "```python\n",
    "class GradientTape(object):\n",
    "  \"\"\"Record operations for automatic differentiation.\n",
    "\n",
    "  Operations are recorded if they are executed within this context manager and\n",
    "  at least one of their inputs is being \"watched\".\n",
    "\n",
    "  Trainable variables (created by `tf.Variable` or `tf.compat.v1.get_variable`,\n",
    "  where `trainable=True` is default in both cases) are automatically watched.\n",
    "  Tensors can be manually watched by invoking the `watch` method on this context\n",
    "  manager.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "dy_dx = g.gradient(y, x) # Will compute to 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=14, shape=(), dtype=float32, numpy=6.0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-tf2",
   "language": "python",
   "name": "py3-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
