{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "### padding mask\n",
    "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise.\n",
    "\n",
    "mask 序列中的 pad tokens. 保证模型不会把 pad token 当做输入。mask indicates 在 pad 的位置输出 1，其他位置输出 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    \n",
    "    # add extra dimensions so that we can add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # [batch_size, 1, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里增加两个 newaxis  主要是为了后面的计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1, 1, 5)\n",
      "tf.Tensor(\n",
      "[[[[0. 0. 1. 1. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 1. 1. 0. 0.]]]], shape=(3, 1, 1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "ans = create_padding_mask(x)\n",
    "print(ans.shape)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look-ahead mask\n",
    "\n",
    "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n",
    "\n",
    "This means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, second and the third word will be used and so on.\n",
    "\n",
    "在 decoder 时，预测下一个词的时候需要 mask 序列中之后的词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask   # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.16677237 0.39485312 0.5738574 ]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=28, shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.linalg.band_part\n",
    "```python\n",
    "tf.linalg.band_part(\n",
    "    input,\n",
    "    num_lower,\n",
    "    num_upper,\n",
    "    name=None\n",
    ")\n",
    "\n",
    "\n",
    "band[i, j, k, ..., m, n] = in_band(m, n) * input[i, j, k, ..., m, n]\n",
    "\n",
    "in_band(m, n) = (num_lower < 0 || (m-n) <= num_lower)) && (num_upper < 0 || (n-m) <= num_upper).\n",
    "```\n",
    "\n",
    "input:要输入的张量tensor.\n",
    "\n",
    "num_lower:下三角矩阵保留的副对角线数量，从主对角线开始计算，相当于下三角的带宽。取值为负数时，则全部保留，矩阵不变。\n",
    "\n",
    "num_upper:上三角矩阵保留的副对角线数量，从主对角线开始计算，相当于上三角的带宽。取值为负数时，则全部保留，矩阵不变。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_a = tf.constant([[ 0,  1,  2, 3],\n",
    "                       [-1,  0,  1, 2],\n",
    "                       [-2, -1,  0, 1],\n",
    "                       [-3, -2, -1, 0]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=33, shape=(4, 4), dtype=float32, numpy=\n",
       "array([[ 0.,  1.,  2.,  3.],\n",
       "       [-1.,  0.,  1.,  2.],\n",
       "       [ 0., -1.,  0.,  1.],\n",
       "       [ 0.,  0., -1.,  0.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.linalg.band_part(input_a, 1, -1)  # 下三角保留一个带宽，上三角全部保留"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=37, shape=(4, 4), dtype=float32, numpy=\n",
       "array([[ 0.,  1.,  0.,  0.],\n",
       "       [-1.,  0.,  1.,  0.],\n",
       "       [-2., -1.,  0.,  1.],\n",
       "       [ 0., -2., -1.,  0.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.linalg.band_part(input_a, 2, 1)  # 下三角保留2个带宽，上三角保留 1 个带宽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look-ahead mask\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask   # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=44, shape=(5, 5), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.linalg.band_part(tf.ones((5,5)), -1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=53, shape=(5, 5), dtype=float32, numpy=\n",
       "array([[0., 1., 1., 1., 1.],\n",
       "       [0., 0., 1., 1., 1.],\n",
       "       [0., 0., 0., 1., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_ahead_mask = create_look_ahead_mask(5)\n",
    "look_ahead_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对 decoder 阶段的第一种attention，也就是 self-attention，既包括 padding mask，也包括 look-ahead mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=200, shape=(2, 4), dtype=float32, numpy=\n",
       "array([[1., 2., 3., 0.],\n",
       "       [2., 3., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt = tf.constant([[ 1,2,3,0],\n",
    "                   [2,3,0,0]], dtype=tf.float32)\n",
    "tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=221, shape=(4, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 1., 1.],\n",
       "       [0., 0., 1., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_ahead_mask = create_look_ahead_mask(tf.shape(tgt)[1])   # [tgt_seq_len, tgt_seq_len] 上三角都为1，下三角（包括对角线）都为0\n",
    "look_ahead_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=229, shape=(2, 1, 1, 4), dtype=float32, numpy=\n",
       "array([[[[0., 0., 0., 1.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 1., 1.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_targrt_padding_mask = create_padding_mask(tgt)  # [batch, 1, 1, tgt_seq_len]\n",
    "dec_targrt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=231, shape=(2, 1, 4, 4), dtype=float32, numpy=\n",
       "array([[[[0., 1., 1., 1.],\n",
       "         [0., 0., 1., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.]]],\n",
       "\n",
       "\n",
       "       [[[0., 1., 1., 1.],\n",
       "         [0., 0., 1., 1.],\n",
       "         [0., 0., 1., 1.],\n",
       "         [0., 0., 1., 1.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_mask = tf.maximum(dec_targrt_padding_mask, look_ahead_mask)\n",
    "combined_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformer 中的三种 mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(inp, tgt):\n",
    "    \"\"\" 这里总共要计算三种 masking，\n",
    "    第一种是 encoding 阶段对于 source 端的 padding  mask\n",
    "    第二种是 decoder 阶段 self-attention 部分的 masked-multi-head-attention,这里不仅包括 padding mask，还包括 look ahead mask\n",
    "    第三种是 decoder 阶段 encoder_ouput 和 attented target 的交互 attention,这里仅仅包括 padding mask，而且与第一阶段一致，因为是 encoder output\n",
    "        作为 key 和 value 值. attented target 是 query.\n",
    "\n",
    "    :param inp:  [batch, inp_seq_len]\n",
    "    :param tgt:  [batch, tgt_seq_len]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 第一种masking: encoding padding mask\n",
    "    enc_padding_amsk = create_padding_mask(inp)  # [batch, 1, 1, inp_seq_len]\n",
    "\n",
    "    # 第三种masking: decoding padding mask\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)  # [batch, 1, 1, inp_seq_len]\n",
    "\n",
    "    # 第二种masking: combined mask\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by\n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tgt)[1])   # [tgt_seq_len, tgt_seq_len] 上三角都为1，下三角（包括对角线）都为0\n",
    "    dec_targrt_padding_mask = create_padding_mask(tgt)           # [batch, 1, 1, tgt_seq_len]\n",
    "    combined_mask = tf.maximum(dec_targrt_padding_mask, look_ahead_mask)    # [batch, 1, tgt_seq_len, tgt_seq_len]\n",
    "    # 对 [batch, 1,1, tgt_Seq_len] 第三个维度广播之后,然后再进行计算。不仅mask了future信息，还mask对应序列中的padding词\n",
    "    return enc_padding_amsk, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tf-2.0",
   "language": "python",
   "name": "tf-2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
