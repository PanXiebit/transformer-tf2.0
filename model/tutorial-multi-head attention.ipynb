{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot product attention\n",
    "![](https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png)\n",
    "\n",
    "输入是 Q (query), K (key), V (value)， 输出是：\n",
    "$$Attention(Q,K,V)=softmax_k(\\dfrac{QK^T}{\\sqrt{(d_k)}})V$$\n",
    "\n",
    "- Q: [batch, q_len, d_model]\n",
    "- K: [batch, kv_len, d_model]\n",
    "- V: [batch, kv_len, d_model]\n",
    "\n",
    "> The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax\n",
    "\n",
    "这里除以 d_model 是避免向量内积之后数量级太大。softmax 之前各个值的大小差距太大(方差变大)，而通过 softmax 之后得到的 probability 变化不大，导致梯度变化很小，以至于 softmax 保持不变(hard softmax).\n",
    "\n",
    "> For example, consider that Q and K have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of dk. Hence, square root of dk is used for scaling (and not any other number) because the matmul of Q and K should have a mean of 0 and variance of 1, so that we get a gentler softmax.\n",
    "\n",
    "$$q\\cdot k=\\sum_{i=1}^{d_k}q_ik_i$$\n",
    "\n",
    "> The mask is multiplied with -1e9 (close to negative infinity). This is done because the mask is summed with the scaled matrix multiplication of Q and K and is applied immediately before a softmax. The goal is to zero out these cells, and large negative inputs to softmax are near zero in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\" caculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    The mask has different shapes depending on its type(padding or look ahead)\n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    :param q: query shape == [..., q_len, d_model]\n",
    "    :param k: key shape == [..., kv_len, d_model]\n",
    "    :param v: value shape == [..., kv_len, d_model]\n",
    "    :param mask: Float tensor with shape broadcastable to [..., q_len, kv_len]\n",
    "    :return:\n",
    "        output, attention_weights.\n",
    "    \"\"\"\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # [..., q_len, kv_len]\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * 1e-9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., q_len, kv_len)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # [.., q_len, d_model] ? [.., k_len, d_model]\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里比较疑惑的是最后的输出 output shape 感觉应该是 `[..., kv_len, d_model]`, 而不是 `[..., q_len, d_model]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "    print ('Attention weights are:')\n",
    "    print (temp_attn)     # [..., len_q, len_kv]\n",
    "    print ('Output is:')\n",
    "    print (temp_out)      # [..., len_q, d_model]\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "temp_k = tf.constant([[10, 0, 0],\n",
    "                      [0, 10, 0],\n",
    "                      [0, 0, 10],\n",
    "                      [0, 0, 10]], dtype=tf.float32)  # (4, 3)\n",
    "temp_v = tf.constant([[1, 0],\n",
    "                      [10, 0],\n",
    "                      [100, 5],\n",
    "                      [1000, 6]], dtype=tf.float32)  # (4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This query aligns with a repeated key (third and fourth),\n",
    "# so all associated values get averaged.\n",
    "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This query aligns equally with the first and second key, \n",
    "# so their values get averaged.\n",
    "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor(\n",
      "[[0.  0.  0.5 0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor(\n",
      "[[550.    5.5]\n",
      " [ 10.    0. ]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**multi-head attention 分成四步：**\n",
    "\n",
    "- Linear layers and split into heads.  \n",
    "- Scaled dot-product attention.  \n",
    "- Concatenation of heads.  \n",
    "- Final linear layer.\n",
    "\n",
    "**为什么要使用 multi-head？**  \n",
    "\n",
    "Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.depth = self.d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "        :param x: [batch_size, seq_len, d_model]\n",
    "        :param batch_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x= tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])   # [batch, num_heads, -1, depth]\n",
    "\n",
    "    def call(self, q, k, v, mask):\n",
    "        \"\"\"\n",
    "\n",
    "        :param v:  [batch, q_len, d_model]\n",
    "        :param k:  [batch, kv_len, d_model]\n",
    "        :param q:  [batch, kv_len, d_model]\n",
    "        :param mask: padding mask or look ahead mask. [..., q_len, kv_len]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        # Linear layers and split into heads\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        split_q = self.split_heads(q, batch_size)\n",
    "        split_k = self.split_heads(k, batch_size)\n",
    "        split_v = self.split_heads(v, batch_size)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        # scaled_attention.shape == [batch, num_heads, q_len, depth]\n",
    "        # attention_weights.shape == [batch, num_heads, q_len, kv_len]\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            split_q, split_k, split_v, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, [0, 2, 1, 3]) # [batch, q_len, num_heads, depth]\n",
    "        \n",
    "        # concatenation\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      shape=(batch_size, -1, self.d_model)) # [batch, q_len, d_model]\n",
    "        \n",
    "        #  Final linear layer\n",
    "        output = self.dense(concat_attention) # [batch, q_len, d_model]\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 62, 512) (1, 8, 62, 60)\n"
     ]
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "q = tf.random.uniform((1, 62, 512))\n",
    "k = v = tf.random.uniform((1, 60, 512))\n",
    "out, attn = temp_mha(q, k, v, mask=None)\n",
    "print(out.shape, attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-tf2",
   "language": "python",
   "name": "py3-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
