{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder and Decoder\n",
    "![](https://www.tensorflow.org/images/tutorials/transformer/transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer model follows the same general pattern as a standard sequence to sequence with attention model.  \n",
    "- The input sentence is passed through N encoder layers that generates an output for each word/token in the sequence.  \n",
    "- The decoder attends on the encoder's output and its own input (self-attention) to predict the next word.\n",
    "\n",
    "仔细看这个图，\n",
    "- 在 encoder 模块，multi-head attention 中的输入 q, k, v 是一致的(self-attention).\n",
    "- 在 deocder 模块，multi-head attention 中的输入 q 来自 target sentence， k，v 来自 source sentence. 所以最后的输出 `output.shape == (batch, q_len, d_model)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder layer\n",
    "\n",
    "Each encoder layer consists of sublayers:\n",
    "\n",
    "- Multi-head attention (with padding mask)  \n",
    "- Point wise feed forward networks.\n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n",
    "\n",
    "The output of each sublayer is LayerNorm(x + Sublayer(x)). The normalization is done on the d_model (last) axis. There are N encoder layers in the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 43, 512)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from multi_head_attention import MultiHeadAttention, point_wise_feed_forward_network\n",
    "\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # [batch, seq_len, d_model]\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "    sample_encoder_layer_output = sample_encoder_layer(\n",
    "        tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "    print(sample_encoder_layer_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each decoder layer consists of sublayers:\n",
    "\n",
    "- Masked multi-head attention (with look ahead mask and padding mask)  \n",
    "- Multi-head attention (with padding mask). V (value) and K (key) receive the encoder output as inputs. Q (query) receives the output from the masked multi-head attention sublayer.  \n",
    "- Point wise feed forward networks\n",
    "\n",
    "decoder 和 encoder 的区别是 decoder 有两个 attenion 模块，分别是：\n",
    "- masked multi-head attention(这部分 q,k,v 是一样的) 不仅包括 padding mask，还包括 look ahead mask. \n",
    "- multi-head attention 这部分 query 是来自上一层的 masked multi-head attention sublayer 的输出， key, value 是来自 encoder 的输出，所以这里只包括 padding mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tf-2.0",
   "language": "python",
   "name": "tf-2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
